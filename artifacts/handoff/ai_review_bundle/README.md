# Project Analysis ‚Äî Macro Health Monitor (hton_project)

> Date: 2026-02-02  
> Scope requested: ‚Äúgo through every folder and every file‚Äù (Option C)  
> Note: This report reflects what could be inspected within the current scanning pass. Some files (notably `backend/engines/*.py`) could not be opened due to tool-call limits and are listed in ‚ÄúRemaining items to inspect‚Äù.

---

## Judge-facing summary (copy/paste friendly)

### What it is

An **offline scouting report + evidence console** for League of Legends match telemetry.

- **Start Demo**: open a top moment + internal evidence panel (match-scoped context)
- **Next**: open scouting report tendencies + click into example evidence
- **Verification**: prove the demo pack is internally consistent (`broken_refs == 0`)

### Why it wins (Coaching Value + ROI)

Coaches spend a lot of time doing manual prep: scanning VODs, extracting tendencies, and rebuilding proof for players.

This project compresses that workflow into **~60‚Äì90 seconds of deterministic clicks** (demo mode), while staying judge-proof:

- zero internet
- no runtime LLM calls
- no heavy compute at runtime (precomputed into the demo pack)
- every insight is **click-to-verify evidence**

More detail: `COACHING_VALUE.md`.

### Validation (honest)

The demo pack is **6 matches** and we describe outputs as **observed tendencies in the demo dataset**, not league-wide truth.

Write-up: `VALIDATION.md`.

### Devpost-ready text (paste this)

**Project:** Offline Scouting Report + Evidence Console

**One-liner:** We turn match telemetry into critical moments + scouting tendencies with click-to-verify evidence ‚Äî fully offline, deterministic, and stage-safe.

**What‚Äôs offline & deterministic:**
- Demo UI calls only local `/api/demo/*` endpoints.
- Backend loads only from `DEMO_PACK_ROOT`.
- Demo pack (`artifacts/demo_pack.tar.gz`) builds bit-for-bit deterministically.

**How the demo works (stage-safe path):**
- Click `Start Demo` ‚Üí opens a top moment and its evidence drawer.
- Click `Next` ‚Üí opens the scouting report and a pattern evidence example.
- Open `Integrity` ‚Üí shows `All insights verified ‚úì` and `broken_refs: 0`.

**Why it‚Äôs trustworthy:**
- Every insight has an internal evidence panel with a **match-scoped ¬±60s context window**.
- Offline verifier embedded in the pack: `artifacts/demo_pack/verify_integrity.py`.

**Limitations (explicit):**
- Demo dataset is 6 matches; confidence labels are conservative (`n=6` ‚Üí LOW).
- This demo is about reliability + evidence trails, not league-wide statistical claims.

Backup demo talk-track: `DEMO_VIDEO_SCRIPT.md`.

### Screenshots for README / Devpost (4 files)

We don‚Äôt commit binary screenshots to git by default, but we provide an automated capture.

1) Start backend + frontend in demo mode (see ‚ÄúDemo mode‚Äù section below)

2) Capture screenshots via Playwright:

macOS/Linux:

```sh
cd frontend
CAPTURE_SCREENSHOTS=1 npm run e2e:capture
```

Windows PowerShell:

```powershell
cd frontend
$env:CAPTURE_SCREENSHOTS="1"
npm run e2e:capture
```

Outputs (repo root):

- `screenshots/01_landing.png`
- `screenshots/02_start_demo_evidence.png`
- `screenshots/03_scouting_report.png`
- `screenshots/04_verification.png`

---

## 1) What the project is (one-paragraph summary)

Macro Health Monitor is a League of Legends macro analytics and coaching support tool. It processes GRID ‚Äúseries state‚Äù match telemetry (positions/events) and produces risk scoring, spatial analytics (cohesion/isolation), pattern detection, and coaching insights. The system consists of a Python FastAPI backend that parses match data and runs analysis ‚Äúengines‚Äù, and a React (Vite) frontend that visualizes timelines, heatmaps, alerts, and batch match trends.

---

## 2) Tech stack & dependencies

### Backend (Python)
- FastAPI + Uvicorn for the API layer
- NumPy + Pandas for analytics
- httpx for HTTP client calls (GRID API)
- python-dotenv for environment configuration
- python-multipart for file upload endpoints
- pytest for test suite
- plotly present as a dependency (also used for HTML dashboard export under `data/processed`)

Python dependencies are declared in `requirements.txt`.

### Frontend (Node / React)
From `frontend/package.json`:
- React 18, React DOM
- Vite build tooling
- TailwindCSS + PostCSS
- Axios for API calls
- React Router for routing
- Recharts + Plotly for visualization

---

## 3) Repository structure (purpose of each folder)

### `backend/`
FastAPI app plus ‚Äúengines‚Äù and parsers.
- `backend/main.py`: API entrypoint + parsing pipeline + endpoints.
- `backend/parsers/grid_parser.py`: helper functions to load GRID-like JSON and extract player positions/events.
- `backend/engines/`: analysis modules (risk, spatial, patterns, insights, heatmaps, validation, etc.).  
  **Status:** folder enumerated; engine file contents were not all inspected in this pass (see Remaining items).

### `core/`
Shared backend ‚Äúlibrary‚Äù layer for config/auth/client/data models and (older) analytics utilities.
- `core/config.py`: loads `.env`, expects `GRID_API_KEY`, ensures `data/raw/` exists. Calls `Config.validate()` at import time.
- `core/auth.py`: builds the request headers for GRID API calls (uses `x-api-key`).
- `core/client.py`: async GRID client with retry/backoff; maps responses into Pydantic models and saves a JSONL snapshot to `data/raw`.
- `core/schemas.py`: Pydantic models (`Position`, `Player`, `GameFrame`, `GameEvent`, `SeriesData`).
- `core/analytics.py`: contains standalone analytics logic (cohesion score, teamfight detection, isolation heuristic, causal chain builder) and a CLI-style `run_analysis()` that reads `data/raw/real_data.json` and writes `data/processed/analytics_report.json`.

**Note:** Config validation is now explicit (no import-time failure if `GRID_API_KEY` is missing). Directory creation is handled via `Config.ensure_data_dirs()`.

### `frontend/`
React UI with Vite + Tailwind + chart components.
- `frontend/src/App.jsx` and `frontend/src/main.jsx` exist (not inspected for content in this pass).
- Component inventory (from filenames):
  - Dashboard, Timeline, RiskScore, CohesionChart
  - AlertPanel / AlertHistory
  - PatternLibrary
  - TeamfightDetails
  - Heatmap
  - CoachingInsights
  - CausalChain + WaterfallChart
  - BatchAnalysis
- API integration: `frontend/src/services/api.js` exists (not inspected for content in this pass).

**Repo hygiene note:** `frontend/node_modules/`, `frontend/dist/`, and `frontend/build/` are build/install artifacts and are ignored via root `.gitignore`.

### `tests/`
pytest suite with unit tests targeting the backend engines/parsers.
- `tests/test_analytics.py`
- `tests/unit/test_*` covers causal analyzer, parser, insight generator, pattern detector, risk calculator, spatial analyzer.
- `tests/integration/` contains API-level endpoint smoke tests.

### `data/`
Project data inputs/outputs.
- `data/raw/real_data.json` ‚Äî a raw sample dataset.
- `data/processed/analytics_report.json`, `risk_timeline.json`, `validation_report.json`, `risk_dashboard.html` ‚Äî generated artifacts for demo/visualization/reporting.

**Repo hygiene note:** `data/processed/` is treated as generated output and is ignored via root `.gitignore`.

---

## Quickstart (judges / fresh machine)

From the repository root:

```sh
make setup
make test
make lint
```

### Demo mode (offline, deterministic, no runtime LLM)

This project supports a **frozen demo pack** that can be generated ahead of time and then loaded with **zero compute** at runtime.

#### 1) Generate the demo pack

```sh
# 6 deterministic demo matches derived from the included sample snapshot
python scripts/generate_demo_matches.py --frames 120

# Build frozen stores + evidence panels and package as a single artifact
python scripts/build_demo_pack.py --source auto
```

Dataset honesty:
- By default, `--source auto` will **prefer real match exports** from `data/demo_matches_real/` if 6 files are present.
- If no real exports are present, it falls back to the bundled synthetic demo inputs in `data/demo_matches/`.
- To force real-only (and fail fast if missing):

```sh
python scripts/build_demo_pack.py --source real
```

This produces:
- `artifacts/demo_pack/` (directory)
- `artifacts/demo_pack.tar.gz` (single artifact)

#### 2) Verify integrity (must pass offline)

```sh
tar -xzf artifacts/demo_pack.tar.gz -C artifacts
python artifacts/demo_pack/verify_integrity.py --pack-root artifacts/demo_pack
```

Expected output:

```text
OK: All integrity checks passed
```

#### 3) Start the app in demo mode

Backend (FastAPI):

```sh
# DEMO_PACK_ROOT points at the extracted demo pack directory
# macOS/Linux:
export DEMO_PACK_ROOT=artifacts/demo_pack
python -m uvicorn backend.main:app --host 0.0.0.0 --port 8000
```

Windows PowerShell:

```powershell
$env:DEMO_PACK_ROOT="artifacts\\demo_pack"
python -m uvicorn backend.main:app --host 0.0.0.0 --port 8000
```

Frontend (React/Vite):

```sh
cd frontend

# Enable selector-first demo UI
export REACT_APP_DEMO_MODE=true
npm run dev
```

Windows PowerShell:

```powershell
cd frontend
$env:REACT_APP_DEMO_MODE="true"
npm run dev
```

#### 3b) Determinism proof (double-build)

CI-grade (recommended; runs determinism self-check + offline verifier + pytest + backend+frontend + Playwright E2E):

```powershell
# Windows
powershell -NoProfile -ExecutionPolicy Bypass -File scripts\run_ci_demo.ps1
```

```sh
# Linux/macOS/CI
bash scripts/run_ci_demo.sh
```

Pack-only determinism checks:

```sh
pytest -q tests/contract/test_demo_pack_determinism.py
pytest -q tests/unit/test_demo_pack_deterministic_tar.py
python scripts/verify_determinism_twice.py
```

Manual (PowerShell) hash double-build snippet (copy/paste safe):

```powershell
python scripts\build_demo_pack.py
$sha1 = (Get-FileHash -Algorithm SHA256 -Path "artifacts\demo_pack.tar.gz").Hash.ToLowerInvariant()
python scripts\build_demo_pack.py
$sha2 = (Get-FileHash -Algorithm SHA256 -Path "artifacts\demo_pack.tar.gz").Hash.ToLowerInvariant()
Write-Host ("sha_run1=$sha1")
Write-Host ("sha_run2=$sha2")
if ($sha1 -ne $sha2) { throw "DETERMINISM: FAIL" } else { Write-Host "DETERMINISM: PASS" -ForegroundColor Green }
```

#### 3c) One-command smoke (optional)

macOS/Linux:

```sh
bash scripts/smoke_demo.sh
```

Windows PowerShell:

```powershell
scripts\smoke_demo.ps1
```

#### Backup demo video checklist (90‚Äì120s)

If anything goes wrong on-stage, have a short backup recording ready:

1. Cold start backend + frontend in demo mode
2. In the UI, click `Start Demo` (evidence drawer should open)
3. Click `Next` (scouting report + pattern evidence should open)
4. Open the integrity panel and show `broken_refs: 0`
5. (Optional) Mention Context Efficiency + Benchmarks panels (both are precomputed at build time)

Store the recording as `submission_assets/backup_demo.mp4` (or your team‚Äôs preferred location).

#### Backup demo video narration script (90‚Äì120s)

Use this as a short, honest voiceover while recording:

1. ‚ÄúThis is `Macro Health Monitor` running in **demo mode**. The demo is **fully offline** and **deterministic**‚Äîno network calls and no runtime LLM/compute.‚Äù
2. ‚ÄúWe generate a frozen `demo_pack` artifact ahead of time. At runtime, the app only loads precomputed JSON stores from `DEMO_PACK_ROOT`.‚Äù
3. ‚ÄúI‚Äôll click `Start Demo` to load a hero match, show 3‚Äì5 coach-plausible moments, and open the top moment‚Äôs evidence drawer.‚Äù
4. ‚ÄúEach claim is evidence-first: you can click into the exact `evidence_id` and inspect the ¬±60s match-scoped context window.‚Äù
5. ‚ÄúNow `Next` opens the scouting report and drills into the first pattern with its supporting evidence.‚Äù
6. ‚ÄúFinally, the integrity panel shows `broken_refs: 0`, proving there are no missing references in the pack.‚Äù
7. ‚ÄúIf anything is corrupted, the API and UI provide explicit remediation steps‚Äîrebuild the demo pack and re-extract it.‚Äù

#### Troubleshooting (demo mode)

- Backend shows `Demo pack corrupted` / `DEMO_PACK_ROOT is not set`
  - Fix: extract the pack and set `DEMO_PACK_ROOT` to the extracted folder.
    - macOS/Linux: `export DEMO_PACK_ROOT=artifacts/demo_pack`
    - PowerShell: `$env:DEMO_PACK_ROOT="artifacts\\demo_pack"`
  - Then re-run: `python -m uvicorn backend.main:app --host 0.0.0.0 --port 8000`
- `python artifacts/demo_pack/verify_integrity.py ...` fails
  - Fix: rebuild and re-extract:
    - `python scripts/generate_demo_matches.py --frames 120`
    - `python scripts/build_demo_pack.py`
    - `tar -xzf artifacts/demo_pack.tar.gz -C artifacts`
- UI says `Backend not reachable`
  - Fix: start the backend first (demo mode requires `DEMO_PACK_ROOT`), then reload the browser.
- Integrity shows `broken_refs > 0`
  - Fix: the demo pack is incomplete or corrupted; rebuild/re-extract and re-run the integrity verifier.

#### Deterministic event ordering (documentation)

Events are sorted deterministically before sequencing. The locked sort key is:

1. `timestamp` (`ts`)
2. `event_type`
3. canonical JSON hash of `payload` (stable across dict ordering)
4. `raw_index` (frame index; last-resort tie-breaker)

Demo endpoints:
- `GET /api/demo/matches`
- `GET /api/demo/teams`
- `GET /api/demo/show-moments?match_id=...`
- `GET /api/demo/analyze-moment?evidence_id=...`
- `GET /api/demo/scout-team?team_id=...`
- `GET /api/demo/integrity`
- `GET /api/demo/observation-masking` (Context Efficiency metric)
- `GET /api/demo/benchmarks` (build-time benchmark report)

#### Windows note
On Windows, `make` may not be installed by default. The Makefile targets map to:

```powershell
python -m venv .venv
.\.venv\Scripts\python -m pip install -r requirements.txt
cd frontend; npm ci
cd ..; .\.venv\Scripts\python -m pytest
.\.venv\Scripts\python -m ruff check .
```

### `archive/`
Legacy code and old logs.
- `archive/legacy/*.py` ‚Äî older pipeline scripts (pre-refactor).
- `archive/logs/*.txt` ‚Äî previous run logs/outputs (not content-inspected here).

### `docs/`
Documentation for data schema/validation.
- `docs/grid_schema.md`
- `docs/validation.md`

### Process / misc
- `.cursor/` and `.bmad-lite/`: editor/process prompt templates and workflow notes.
- `.venv/`: local Python virtual env (should not be committed).
- `logs/`, `screenshots/`: exist; appear empty at the time of scanning.

---

## 4) Backend API behavior (confirmed from `backend/main.py`)

### Endpoints
- `GET /api/health`: health check.
- `POST /api/parse-match`: upload a single match file (JSON or JSONL); returns an `analytics` payload.
- `POST /api/analyze-batch`: upload multiple files; returns aggregate stats + per-match analytics + validation.
- `GET /api/export-csv`: returns a CSV download (currently demo/static rows).

### Core pipeline (`process_match_data`)
- Accepts either JSONL (one JSON object per line) or JSON (single object or list of objects).
- Limits to the first ~100 frames for demo performance.
- Extracts positions via `extract_player_positions`.
- Computes:
  - timeline entries (game time, mocked gold diff, risk score)
  - cohesion history
  - pattern detection history
  - teamfight windows (heuristic, with mocked ‚Äúwinner‚Äù)
  - isolation alerts (last frame, for the red team vs blue team)
  - coaching insights (based on history + final risk score)
  - causal chain and heatmaps (currently hardcoded/demo-based inputs)

### Bugs / correctness issues observed
- **Unreachable caching code / undefined variable:**  
  `process_match_data` returns a dict, and after that there is code intended to store `result` in a cache ‚Äî but `result` is not defined and that block is unreachable due to the earlier return. Effectively: caching as written does not work and is a maintenance trap.

### Demo-mode signals
Several values appear mocked/demo-driven (e.g., gold diff curve, causal chain entries, heatmap points, teamfight ‚Äúwon‚Äù logic). This is fine for a demo, but should be made explicit via a ‚Äúdemo mode‚Äù flag or separated into a simulator layer.

---

## 5) Core runtime / data ingestion (confirmed from `core/*`)

### Configuration (`core/config.py`)
- Loads `.env` from project root.
- Requires `GRID_API_KEY`. If missing, import-time validation throws immediately.
- Ensures `data/raw/` exists.

**Impact:** tests/CI or any runtime without `GRID_API_KEY` set can fail at import time. Consider making validation opt-in (e.g., only on app startup) or allowing a ‚Äúno-network‚Äù mode for tests.

### GRID client (`core/client.py`)
- Async HTTP client (httpx) with:
  - timeouts
  - retry logic with exponential backoff
  - explicit 429 handling
- Tries to fetch ‚Äústate‚Äù and ‚Äúevents‚Äù data and map them into Pydantic models.
- Writes a JSONL file under `data/raw/` containing the serialized `SeriesData` model.

**Potential concern:** the URLs used by `GridClient.fetch_match_data()` look like REST-ish endpoints constructed from a GraphQL base URL. This may be intentional (depending on GRID OP endpoints) or may not match the real API shape. Needs verification.

### Analytics utilities (`core/analytics.py`)
Contains:
- cohesion scoring based on average spread from centroid
- teamfight detection based on player proximity thresholds
- isolation heuristic (‚ÄúLonely Carry Index‚Äù style): carry is isolated if far from allies and close to enemies
- a simple causal chain builder that attributes risk drops to likely causes (heuristic)

This module behaves like a CLI script producing `data/processed/analytics_report.json`.

---

## 6) Frontend structure (from tree)

The frontend has a strong ‚Äúanalytics dashboard‚Äù component layout:
- Timeline visualization
- Cohesion chart
- Risk scoring and stage display
- Heatmaps and hotspots
- Alert panels/history
- Pattern library and teamfight details
- Causal chain / waterfall chart
- Batch analysis UI

**Not yet confirmed:** exact API URL configuration and how responses are mapped (needs reading `frontend/src/services/api.js`, `frontend/src/App.jsx`, etc.).

---

## 7) Tests

The unit test suite indicates intended behavior for:
- parser (`grid_parser`)
- risk calculator
- spatial analyzer
- pattern detector
- insight generator
- causal analyzer

This is a good sign: it implies each ‚Äúengine‚Äù has an expected contract, even if the current `backend/main.py` uses demo inputs.

---

## 8) Repo hygiene / operational risks

### Likely should be git-ignored (currently present)
- `frontend/node_modules/`
- `frontend/dist/`
- Python `__pycache__/`
- `.pytest_cache/`
- `.venv/`

### Config/secrets
- `.env` exists. It should never be committed with real secrets. Treat it as local-only.

### Demo vs production concerns
- Backend analysis currently mixes real parsing with demo/simulated inputs.
- Recommend a clean separation:
  - ‚Äúreal data mode‚Äù (compute all from parsed frames/events)
  - ‚Äúdemo mode‚Äù (synthetic values clearly labeled)

---

## 9) Remaining items to inspect (to truly complete ‚Äúevery file‚Äù)

### Backend engines (not yet opened in this pass)
- `backend/engines/risk_calculator.py`
- `backend/engines/spatial_analyzer.py`
- `backend/engines/pattern_detector.py`
- `backend/engines/insight_generator.py`
- `backend/engines/heatmap_generator.py`
- `backend/engines/causal_analyzer.py`
- `backend/engines/correlation_analyzer.py`
- `backend/engines/validator.py`

### Frontend source contents (not yet opened in this pass)
- `frontend/src/App.jsx`
- `frontend/src/main.jsx`
- `frontend/src/services/api.js`
- all `frontend/src/components/*.jsx`
- `frontend/src/utils/constants.js`

### Root documentation & process files (not yet content-reviewed in this pass)
- `CURRENT-STATUS.md`, `DECISIONS.md`, `PROJECT_GOALS.md`, `plan*.md`, pitch docs, and `docs/*.md` (beyond listing)

---

## 10) Recommended next steps (practical)

1) Fix caching bug in `backend/main.py` (unreachable code / undefined variable).
2) Decide on a ‚Äúdemo mode‚Äù switch and make mocked values explicit.
3) Make configuration validation test-friendly:
   - avoid failing import-time when `GRID_API_KEY` is missing in unit tests.
4) Add/verify `.gitignore` for `node_modules`, `dist`, `.venv`, caches, and generated `data/processed` artifacts as appropriate.
5) Confirm GRID endpoint correctness in `core/client.py` and align with schema expectations in `docs/grid_schema.md`.

---

## Appendix: Quick ‚Äúhow to run‚Äù (based on repo files)

Backend:
- Install Python deps: `pip install -r requirements.txt`
- Run: `uvicorn backend.main:app --reload`

Frontend:
- `cd frontend`
- `npm install`
- `npm run dev`
Macro Health Monitor ist ein High-End Analyse-Tool f√ºr League of Legends Coaches, das auf GRID-Daten basiert. Es transformiert rohe Koordinaten-Daten in strategische Intelligenz, um Risiken vorherzusagen und Spielmuster zu erkennen.

## üåü Key Features (v1.2.0-PRO)
- **Heuristische Risk-Engine:** Echtzeit-Berechnung des Risikos basierend auf Gold-Diff, Vision und Formation.
- **Pattern Recognition:** Automatische Erkennung strategischer Man√∂ver (Baron Setups, 1-4 Split Pushes).
- **Actionable Insights:** KI-gest√ºtzte Coaching-Empfehlungen f√ºr Taktik und Vision-Control.
- **Multi-Match Trends:** Batch-Verarbeitung zur Analyse strategischer Konsistenz √ºber mehrere Serien.
- **Replay Simulation:** Interaktive Wiedergabe von Matches mit Live-Risiko-Update.
- **Spatial Analytics:** Berechnung der Team-Koh√§sion und Isolation (Lonely Carry Index).

## üìä Validation & Accuracy
Das System wurde gegen historische Profi-Matches validiert:
- **Risk Prediction Accuracy:** **87.2%** (Vorhersage des Gewinners ab Minute 15).
- **Correlation Index:** 92% Korrelation zwischen Cohesion-Drops und Spieler-Toden.
- **Pattern Precision:** 95% bei der Erkennung taktischer Standard-Setups.
*Details finden Sie in `docs/validation.md`.*

## üõ†Ô∏è Tech Stack
- **Backend:** Python, FastAPI, NumPy, Pandas, Pytest
- **Frontend:** React, Tailwind CSS, Recharts
- **Data:** GRID Open Access API (JSONL Series States)

## üì¶ Installation & Start

### Backend
1. Python 3.9+ installieren.
2. Abh√§ngigkeiten installieren:
   ```bash
   pip install -r requirements.txt
   ```
3. Server starten:
   ```bash
   uvicorn backend.main:app --reload
   ```

### Frontend
1. Node.js installieren.
2. Im `frontend` Ordner:
   ```bash
   npm install
   npm run dev
   ```

## ü§ñ AI Integration
Dieses Projekt nutzt fortschrittliche AI-Agenten f√ºr:
- **Spatial Clustering Algorithmen** (Cohesion & LCI).
- **Neural Pattern Matching** f√ºr taktische Signaturen.
- **Automated Insight Generation** f√ºr Coaching-Workflows.
